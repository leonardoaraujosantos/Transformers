{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Text Data\n",
    "\n",
    "#### TorchText\n",
    "TorchText help to load/preprocess NLP datasets, you can follow a nice tutorial [here](https://towardsdatascience.com/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster-77f3884d95) and oficial doc is: [here](https://torchtext.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "![alt text](docs/imgs/torchtext_diagram.png \"Title\")\n",
    "\n",
    "Main features of TorchText:\n",
    "* Ability to define a preprocessing pipeline\n",
    "* Batching, padding, and numericalizing (including building a vocabulary object)\n",
    "* Wrapper for dataset splits (train, validation, test)\n",
    "* Loader a custom NLP dataset\n",
    "\n",
    "#### Spacy\n",
    "It's a production library to help NLP tasks, it's main features\n",
    "* Tokenization (What we want now)\n",
    "* Part-of-speech tagging\n",
    "* Similarity\n",
    "* Serialization\n",
    "\n",
    "Spacy is a library that has been specifically built to take sentences in various languages and split them into different tokens.\n",
    "\n",
    "![alt text](docs/imgs/spacy_diagram.png \"Title\")\n",
    "\n",
    "For examples and tutorials check [here](https://spacy.io/usage/spacy-101)\n",
    "\n",
    "#### Tokenizer and Indexing\n",
    "First we need to transform our senteces into tokens and then into indexes of words.\n",
    "\n",
    "![alt text](docs/imgs/tokenizer_indexing.png \"Title\")\n",
    "\n",
    "#### Install spacy/torchtext and language support\n",
    "```bash\n",
    "pip install torchtext spacy\n",
    "# Download \n",
    "python -m spacy download en\n",
    "python -m spacy download de\n",
    "python -m spacy download fr\n",
    "python -m spacy download pt\n",
    "```\n",
    "\n",
    "#### Download Some Datasets\n",
    "``` bash\n",
    "wget http://www.statmt.org/europarl/v7/fr-en.tgz\n",
    "tar -zxvf fr-en.tgz\n",
    "```\n",
    "\n",
    "#### References\n",
    "* https://medium.com/@debanjanmahata85/natural-language-processing-with-spacy-36b90b9afa3d\n",
    "* https://spacy.io/usage/training\n",
    "* [Tutorial on TorchText](http://anie.me/On-Torchtext/)\n",
    "* https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "* https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-i-5da6f1c89d84\n",
    "* https://towardsdatascience.com/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster-77f3884d95\n",
    "* https://github.com/pytorch/text\n",
    "* https://nlpforhackers.io/complete-guide-to-spacy/\n",
    "* http://www.statmt.org/europarl/\n",
    "* [Nice Sentiment Analysis using torchtext](https://medium.com/@sonicboom8/sentiment-analysis-torchtext-55fb57b1fab8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchtext import data, datasets\n",
    "import spacy\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "\n",
    "# Use to split train/val\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download spacy class to handle english and french\n",
    "spacy_fr = spacy.load('fr')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "SOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "BLANK_WORD = \"<blank>\"\n",
    "\n",
    "MAX_LEN = 100\n",
    "MIN_FREQ = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', '!', 'my', 'name', 'is', 'Leo', ',', 'and', 'yours', '?']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_fr(text):\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "print(tokenize_en('Hi! my name is Leo, and yours?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = data.Field(tokenize=tokenize_fr, pad_token=BLANK_WORD)\n",
    "TGT = data.Field(tokenize=tokenize_en, init_token = SOS_WORD, eos_token = EOS_WORD, pad_token=BLANK_WORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "europarl_en = open('./europarl-v7.fr-en.en', encoding='utf-8').read().split('\\n')\n",
    "europarl_fr = open('./europarl-v7.fr-en.fr', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "raw_data = {'English' : [line for line in europarl_en], 'French': [line for line in europarl_fr]}\n",
    "df = pd.DataFrame(raw_data, columns=[\"English\", \"French\"])\n",
    "\n",
    "# remove very long sentences and sentences where translations are \n",
    "# not of roughly equal length\n",
    "df['eng_len'] = df['English'].str.count(' ')\n",
    "df['fr_len'] = df['French'].str.count(' ')\n",
    "df = df.query('fr_len < 80 & eng_len < 80')\n",
    "df = df.query('fr_len < eng_len * 1.5 & fr_len * 1.5 > eng_len')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Between Train/Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and validation set \n",
    "train, val = train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train.csv\", index=False)\n",
    "val.to_csv(\"val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pytorch Dataset\n",
    "Now use the spacy tokenizers and torchtext to process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create source and target fields given the spacy tokenizers\n",
    "SRC = data.Field(tokenize=tokenize_fr, pad_token=BLANK_WORD)\n",
    "TGT = data.Field(tokenize=tokenize_en, init_token = SOS_WORD, eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
    "\n",
    "# associate the text in the 'English' column with the EN_TEXT field, # and 'French' with FR_TEXT\n",
    "data_fields = [('English', TGT), ('French', SRC)]\n",
    "train,val = data.TabularDataset.splits(path='./', train='train.csv', validation='val.csv', format='csv', fields=data_fields)\n",
    "\n",
    "# Other way..\n",
    "#train, val, test = datasets.IWSLT.splits(exts=('.fr', '.en'), fields=(SRC, TGT), \n",
    "#    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get indexes for all words\n",
    "This step will get an specific index for every word, this will be the embedding input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train, val, min_freq=MIN_FREQ)\n",
    "TGT.build_vocab(train, val, min_freq=MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Index of word \\'the\\:', SRC.vocab.stoi['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get an Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = BucketIterator(train, batch_size=10, sort_key=lambda x: len(x.French), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(batch.English)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More efficient way\n",
    "While Torchtext is brilliant, it’s sort_key based batching leaves a little to be desired. Often the sentences aren’t of the same length at all, and you end up feeding a lot of padding into your network (as you can see with all the 1s in the last figure).\n",
    "\n",
    "An efficient batching mechanism would change the batch size depending on the sequence length to make sure around 1500 tokens were being processed each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.English))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.French) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)\n",
    "class MyIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
