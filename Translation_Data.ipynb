{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Text Data\n",
    "\n",
    "#### TorchText\n",
    "TorchText help to load/preprocess NLP datasets, you can follow a nice tutorial [here](https://towardsdatascience.com/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster-77f3884d95)\n",
    "\n",
    "![alt text](docs/imgs/torchtext_diagram.png \"Title\")\n",
    "\n",
    "#### Spacy\n",
    "It's a production library to help NLP tasks, it's main features\n",
    "* Tokenization (What we want now)\n",
    "* Part-of-speech tagging\n",
    "* Similarity\n",
    "* Serialization\n",
    "\n",
    "Spacy is a library that has been specifically built to take sentences in various languages and split them into different tokens.\n",
    "\n",
    "![alt text](docs/imgs/spacy_diagram.png \"Title\")\n",
    "\n",
    "For examples and tutorials check [here](https://spacy.io/usage/spacy-101)\n",
    "\n",
    "#### Tokenizer and Indexing\n",
    "First we need to transform our senteces into tokens and then into indexes of words.\n",
    "\n",
    "![alt text](docs/imgs/tokenizer_indexing.png \"Title\")\n",
    "\n",
    "#### Install spacy/torchtext and language support\n",
    "```bash\n",
    "pip install torchtext spacy\n",
    "# Download \n",
    "python -m spacy download en\n",
    "python -m spacy download de\n",
    "python -m spacy download fr\n",
    "python -m spacy download pt\n",
    "```\n",
    "\n",
    "#### Download Some Datasets\n",
    "``` bash\n",
    "wget http://www.statmt.org/europarl/v7/fr-en.tgz\n",
    "tar -zxvf fr-en.tgz\n",
    "```\n",
    "\n",
    "#### References\n",
    "* https://medium.com/@debanjanmahata85/natural-language-processing-with-spacy-36b90b9afa3d\n",
    "* https://spacy.io/usage/training\n",
    "* http://anie.me/On-Torchtext/\n",
    "* https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "* https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-i-5da6f1c89d84\n",
    "* https://towardsdatascience.com/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster-77f3884d95\n",
    "* https://github.com/pytorch/text\n",
    "* https://nlpforhackers.io/complete-guide-to-spacy/\n",
    "* http://www.statmt.org/europarl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchtext import data, datasets\n",
    "import spacy\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "\n",
    "# Use to split train/val\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download spacy class to handle english and french\n",
    "spacy_fr = spacy.load('fr')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "SOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "BLANK_WORD = \"<blank>\"\n",
    "\n",
    "MAX_LEN = 100\n",
    "MIN_FREQ = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', '!', 'my', 'name', 'is', 'Leo', ',', 'and', 'yours', '?']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_fr(text):\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "print(tokenize_en('Hi! my name is Leo, and yours?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = data.Field(tokenize=tokenize_fr, pad_token=BLANK_WORD)\n",
    "TGT = data.Field(tokenize=tokenize_en, init_token = SOS_WORD, eos_token = EOS_WORD, pad_token=BLANK_WORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "europarl_en = open('./europarl-v7.fr-en.en', encoding='utf-8').read().split('\\n')\n",
    "europarl_fr = open('./europarl-v7.fr-en.fr', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "raw_data = {'English' : [line for line in europarl_en], 'French': [line for line in europarl_fr]}\n",
    "df = pd.DataFrame(raw_data, columns=[\"English\", \"French\"])\n",
    "\n",
    "# remove very long sentences and sentences where translations are \n",
    "# not of roughly equal length\n",
    "df['eng_len'] = df['English'].str.count(' ')\n",
    "df['fr_len'] = df['French'].str.count(' ')\n",
    "df = df.query('fr_len < 80 & eng_len < 80')\n",
    "df = df.query('fr_len < eng_len * 1.5 & fr_len * 1.5 > eng_len')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Between Train/Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and validation set \n",
    "train, val = train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train.csv\", index=False)\n",
    "val.to_csv(\"val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pytorch Dataset\n",
    "Now use the spacy tokenizers and torchtext to process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create source and target fields given the spacy tokenizers\n",
    "SRC = data.Field(tokenize=tokenize_fr, pad_token=BLANK_WORD)\n",
    "TGT = data.Field(tokenize=tokenize_en, init_token = SOS_WORD, eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
    "\n",
    "# associate the text in the 'English' column with the EN_TEXT field, # and 'French' with FR_TEXT\n",
    "data_fields = [('English', TGT), ('French', SRC)]\n",
    "train,val = data.TabularDataset.splits(path='./', train='train.csv', validation='val.csv', format='csv', fields=data_fields)\n",
    "\n",
    "# Other way..\n",
    "#train, val, test = datasets.IWSLT.splits(exts=('.fr', '.en'), fields=(SRC, TGT), \n",
    "#    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get indexes for all words\n",
    "This step will get an specific index for every word, this will be the embedding input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train, val, min_freq=MIN_FREQ)\n",
    "TGT.build_vocab(train, val, min_freq=MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Index of word \\'the\\:', SRC.vocab.stoi['the'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
