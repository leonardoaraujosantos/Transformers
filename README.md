# Introduction
Repo with my attempt to implement transformers networks with Pytorch.

#### References
* http://nlp.seas.harvard.edu/2018/04/03/attention.html
* https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/blob/master/AnnotatedMultiHeadAttention.ipynb
* (Building Transformer XL from scratch)[https://mlexplained.com/2019/07/04/building-the-transformer-xl-from-scratch/]