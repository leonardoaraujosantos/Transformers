# Introduction
Repo with my attempt to implement transformers networks with Pytorch.

#### References
* [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
* https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/blob/master/AnnotatedMultiHeadAttention.ipynb
* [Building Transformer XL from scratch](https://mlexplained.com/2019/07/04/building-the-transformer-xl-from-scratch/)