{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is all you Need Vanilla Transformer\n",
    "Implement \"Attention os all you need\" paper.\n",
    "![alt text](./docs/imgs/IntroTransformer.png \"Title\")\n",
    "\n",
    "#### References\n",
    "* [LayerNorm paper](https://arxiv.org/abs/1607.06450)\n",
    "* [Attention is all you need blog](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XXLlHKeZPys)\n",
    "* [How to code the transformer in pytorch](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec)\n",
    "* [Multiheaded Attention Pytorch Docs](https://pytorch.org/docs/stable/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention)\n",
    "* [Transformer from Pytorch Docs](https://pytorch.org/docs/stable/nn.html?highlight=transformer#torch.nn.Transformer)\n",
    "* [Nice working reference](https://github.com/dreamgonfly/Transformer-pytorch)\n",
    "* [Notebook compatible with Pytorch 1.0](https://github.com/ArdalanM/annotated-transformer/blob/fix_pytorch_1.0_plus/The%20Annotated%20Transformer.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.1.0\n",
      "Device: cuda:0\n",
      "Number of GPUs Available: 8\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import utils_transformer as utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "num_gpu = torch.cuda.device_count()\n",
    "print('Number of GPUs Available:', num_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "sequence_length = 10\n",
    "hidden_size = 32\n",
    "attention_heads = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention\n",
    "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$.   The keys and values are also packed together into matrices $K$ and $V$.  We compute the matrix of outputs as:                      \n",
    "                                                                 \n",
    "$$                                                                         \n",
    "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V               \n",
    "$$   \n",
    "\n",
    "![alt text](./docs/imgs/attention_block.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement attention (Scaled Dot Product)\n",
    "def attention(query, key, value, dropout=None):\n",
    "    d_k = query.size(-1)\n",
    "    key = key.transpose(-2, -1)\n",
    "    scores = torch.matmul(query, key) / math.sqrt(d_k)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    attention_result = torch.matmul(p_attn, value)\n",
    "    return attention_result, p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiHeaded Attention\n",
    "The MultiHeaded attention block allows us to attend to the same concept at different levels of representation (N)\n",
    "![alt text](./docs/imgs/MultiHeaddedAttention_Block.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, hidden_size, linears=True, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert hidden_size % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = hidden_size // h\n",
    "        self.h = h\n",
    "        if linears: \n",
    "            self.linears = utils.clones(nn.Linear(hidden_size, hidden_size), 4)\n",
    "        else:\n",
    "            self.linears = [lambda arg: arg] * 4\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from hidden_size => h x d_k \n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        \n",
    "        # Get result from last linear\n",
    "        x = self.linears[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With as many attention queries as there are values:\n",
      "\n",
      "query: torch.Size([64, 1, 32])\n",
      "value: torch.Size([64, 10, 32])\n",
      "result: torch.Size([64, 1, 32])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadedAttention(h=attention_heads, hidden_size=hidden_size)\n",
    "print(\"With as many attention queries as there are values:\\n\")\n",
    "query = torch.tensor(np.ones([batch_size, 1, hidden_size])).float()\n",
    "value = torch.tensor(np.ones([batch_size, sequence_length, hidden_size])).float()\n",
    "result = mha.forward(query, value, value)\n",
    "print(\"query:\", query.size())\n",
    "print(\"value:\", value.size())\n",
    "print(\"result:\", result.size())\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position Wise Feed Forward\n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.  This consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$                                                                                                                                                                                                      \n",
    "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.  The dimensionality of input and output is $d_{\\text{model}}=512$, and the inner-layer has dimensionality $d_{ff}=2048$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements the sinusoidal positional encoding for non-recurrent neural networks.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # Few changes to force position/div_term to float\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "pe = PositionalEncoding(20, 0)\n",
    "y = pe.forward(torch.zeros(1, 100, 20))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])\n",
    "None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
